{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## This notebook will process all of the data in the antenna result directories and produce a 'processed_data.csv' file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "#### The below cell will install dependencies, if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# ! pip install folium\n",
    "# ! pip install pandas\n",
    "# ! pip install pandas_profiling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "#### The necessary dependencies are imported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pandas_profiling\n",
    "import folium\n",
    "from folium.plugins import HeatMap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "#### Add values for the below variables to return a processed file containing only a subset of the data.\n",
    "##### For example, add a species to return a file of only data for that species. Or, add a species and an antenna to get data for a specific species/antenna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User Selections\n",
    "# Add a pythonic list to any of these for getting a subset of the data for the below analytics functions.\n",
    "# This will not overwrite the processed_data.csv file with the subset.\n",
    "\n",
    "SPECIES = []\n",
    "ANTENNA = []\n",
    "DATE = []\n",
    "TAGID = []\n",
    "COLUMNS = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "#### Execute 'fish_data.py' processing script. This creates the 'processed_data.csv' file. Each time this cell is run it will recreate the 'processed_data.csv' file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing Records from downstream 10.15.txt... \n",
      "Importing Records from downstream 10.2.txt... \n",
      "Importing Records from downstream 6.13.txt... \n",
      "Error processing line: D 2018-06-09 23:59:59.61 ï¿½8:16:01.03 HA 3D6.00184CE0D4    2    20\n",
      "\n",
      "Importing Records from downstream 6.28.txt... \n",
      "Importing Records from downstream 6.8.txt... \n",
      "Error processing line: D 2008-10-18 Z6:42:N1.37 b2:17:I6.04 HB 000.0000000000 30137 25605\n",
      "\n",
      "Error processing line: D 2018-05-12 00:00:05.29 ï¿½2:15:58.06 HA 3D6.00184CB873    2     0\n",
      "\n",
      "Error processing line: D 2018-05-12 00:00:05.19 ï¿½2:15:58.16 HA 3D6.00184CB873    3     4\n",
      "\n",
      "Error processing line: D 2018-05-12 00:00:05.19 ï¿½2:15:58.16 HA 3D6.00184CB873    3     4\n",
      "\n",
      "Error processing line: D 2018-05-12 00:00:46.20 ï¿½2:15:59.31 HA 3D6.00184CB873    4     1\n",
      "\n",
      "Error processing line: D 2018-05-12 00:16:25.72 00.00 HA 3D6.00184CB873    1     1\n",
      "\n",
      "Error processing line: D 2018-05-12 01:45:35.65 ;0:56:01.13 HA 3D6.00184CBA2D    3     1\n",
      "\n",
      "Error processing line: D 2018-05-12 01:45:35.65 ;0:56:01.13 HA 3D6.00184CBA2D    3     1\n",
      "\n",
      "Error processing line: D 2023-01-16 Y0:03:V6.59 00:00:00.00 HA 356.00855146F1    1     0\n",
      "\n",
      "Error processing line: D 2018-05-14 23:59:53.76 ï¿½8:16:17.85 HA 3D6.00184CB873    6     1\n",
      "\n",
      "Error processing line: D 2018-05-15 00:00:12.62 ï¿½2:15:57.98 HA 3D6.00184CB873    8     1\n",
      "\n",
      "Error processing line: D 2018-05-15 00:00:12.62 ï¿½2:15:57.98 HA 3D6.00184CB873    8     1\n",
      "\n",
      "Error processing line: D 2018-05-15 00:00:12.62 ï¿½2:15:57.98 HA 3D6.00184CB873    8     1\n",
      "\n",
      "Error processing line: D 2018-05-17 23:59:44.58 ï¿½8:16:35.16 HA 3D6.00184CBAF8   35     4\n",
      "\n",
      "Error processing line: D 20  1     2\n",
      "\n",
      "Importing Records from downstream 7.17.txt... \n",
      "Importing Records from downstream 7.27.txt... \n",
      "Error processing line: D 2018-07-22 23:59:56.01 š8:16:12.40 HA 3D6.00184CE0AB   13    51\n",
      "\n",
      "Error processing line: D 2018-07-23 04:58:03.86 00:00:00.00 HA 20\n",
      "\n",
      "Importing Records from downstream 8.16.txt... \n",
      "Error processing line: D 2018-08-06 23:59:52.43 š8:16:23.78 HA 3D6.00184CB8B0   24     1\n",
      "\n",
      "Error processing line: D 2018-08-13 23:59:57.48 š8:16:03.10 HA 3D6.00184CBAC6    4     1\n",
      "\n",
      "Importing Records from downstream 8.2.txt... \n",
      "Importing Records from downstream 8.6.txt... \n",
      "Importing Records from downstream 9.18.txt... \n",
      "Error processing line: D 2018-09-15 21:44:21.38 00:00:00.00 H018-09-15 23:06:35.75 00:00:00.00 HA 3D6.00184CBB1C    1    18\n",
      "\n",
      "Importing Records from downstream 9.25.txt... \n",
      "Importing Records from downstream 9.7.txt... \n",
      "Error processing line: D 2018-08-28 22:16:06.45 00:00:01.03 HA 3D6.00184CBA07    2   29 00:29:20.03 00:00:00.00 HA 3D6.00184CB8AF    1     6\n",
      "\n",
      "Importing Records from upstream 1 10.2.txt... \n",
      "Importing Records from upstream 1 6.28.txt... \n",
      "Error processing line: D 2018-05-31 23:59:57.14 š8:16:21.71 HA 3D6.00184CB876   22     1\n",
      "\n",
      "Importing Records from upstream 1 7.17.txt... \n",
      "Importing Records from upstream 1 7.27.txt... \n",
      "Importing Records from upstream 1 8.16.txt... \n",
      "Importing Records from upstream 1 8.2.txt... \n",
      "Importing Records from upstream 1 8.21.txt... \n",
      "Importing Records from upstream 1 8.6.txt... \n",
      "Importing Records from upstream 1 9.18.txt... \n",
      "Importing Records from upstream 1 9.25.txt... \n",
      "Importing Records from upstream 1 9.7.txt... \n",
      "Importing Records from upstream 2 10.15.txt... \n",
      "Importing Records from upstream 2 6.13.txt... \n",
      "Importing Records from upstream 2 6.28.txt... \n",
      "Importing Records from upstream 2 7.17.txt... \n",
      "Importing Records from upstream 2 7.27.txt... \n",
      "Importing Records from upstream 2 8.16.txt... \n",
      "Importing Records from upstream 2 8.2.txt... \n",
      "Importing Records from upstream 2 8.6.txt... \n",
      "Error processing line: D 2018-08-05 23:59:53.25 š8:16:07.24 HA 3D6.00184CB93D    8    12\n",
      "\n",
      "Importing Records from upstream 2 9.18.txt... \n",
      "Importing Records from upstream 3 10.15.txt... \n",
      "Importing Records from upstream 3 10.2.txt... \n",
      "Importing Records from upstream 3 7.17.txt... \n",
      "Importing Records from upstream 3 7.27.txt... \n",
      "Importing Records from upstream 3 8.2.txt... \n",
      "Importing Records from upstream 3 9.11.txt... \n",
      "Importing Records from upstream 3 9.18.txt... \n",
      "Importing Records from upstream 3 9.25.txt... \n",
      "Importing Records from upstream 3 9.7.txt... \n",
      "Complete.\n",
      "Processed 423395 records.\n",
      "Wrote 295 records after deduplication.\n",
      "Error reading 0 records\n",
      "Error processing 24 records\n"
     ]
    }
   ],
   "source": [
    "%run -i fish_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "#### The cell below does all of the heavy lifting of reading the 'processed_data.csv' file into a dataframe for use in the operations below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare dataframe for analysis\n",
    "\n",
    "# Load fish tag data into a dataframe\n",
    "fish_tag_data = pd.read_csv('./tag_data.csv', \n",
    "    names=['Date','Time','Tag ID','Species','Length','Capture Method','Marked At'], low_memory=False)\n",
    "\n",
    "# Load antenna data into a dataframe\n",
    "data = pd.read_csv('./processed_data.csv', \n",
    "    names=['D','Date','Time','Duration','Type','Tag ID','Count','Gap','Antenna'], low_memory=False)\n",
    "\n",
    "# Join Dataframe on Tag ID\n",
    "data = pd.merge(data,fish_tag_data[['Tag ID', 'Species', 'Length', 'Marked At']],on='Tag ID', how='left')\n",
    "\n",
    "# Alter dtypes for time fields\n",
    "# data['Date'] = data['Date'].astype('datetime64[ns]')\n",
    "# data['Time'] = pd.to_timedelta(data['Time'])\n",
    "\n",
    "# Antennae Lat/Long GLOBALS\n",
    "U1_LAT=33.99644444\n",
    "U1_LONG=-84.89666667\n",
    "U2_LAT=33.99697222\n",
    "U2_LONG=-84.89694444\n",
    "U3_LAT=33.99700000\n",
    "U3_LONG=-84.89805556\n",
    "D1_LAT=33.99852778\n",
    "D1_LONG=-84.89444444\n",
    "\n",
    "# Add Lat/Long information to DATAFRAME\n",
    "data.loc[data.Antenna == 'U1','lat'] = U1_LAT\n",
    "data.loc[data.Antenna == 'U1','long'] = U1_LONG\n",
    "data.loc[data.Antenna == 'U2','lat'] = U2_LAT\n",
    "data.loc[data.Antenna == 'U2','long'] = U2_LONG\n",
    "data.loc[data.Antenna == 'U3','lat'] = U3_LAT\n",
    "data.loc[data.Antenna == 'U3','long'] = U3_LONG\n",
    "data.loc[data.Antenna == 'D1','lat'] = D1_LAT\n",
    "data.loc[data.Antenna == 'D1','long'] = D1_LONG\n",
    "\n",
    "# Fill all missing values with a zero\n",
    "\n",
    "print(data.sample())\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "#### The below cell creates the subset dataframe from the selection provided by the user above. It will perform the heatmapping and the pandas_profiling from this subset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a data subset, if desired.\n",
    "\n",
    "if SPECIES:\n",
    "    data = data.loc[data['Species'].isin(SPECIES)]\n",
    "if ANTENNA:\n",
    "    data = data.loc[data['Antenna'].isin(ANTENNA)]\n",
    "if DATE:\n",
    "    data = data.loc[data_subset['Date'].isin(DATE)]\n",
    "if TAGID:\n",
    "    data = data.loc[data_subset['Tag ID'].isin(TAGID)]\n",
    "if COLUMNS:\n",
    "    data = data.filter(COLUMNS, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_profiling.ProfileReport(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fishTrackMap = folium.Map(location=[33.99697222, -84.89694444], zoom_start=15) \n",
    "\n",
    "# Ensure floats\n",
    "data['lat'] = data['lat'].astype(float)\n",
    "data['lat'] = data['lat'].astype(float)\n",
    "\n",
    "# Remove NaNs\n",
    "data = data[['lat', 'long']]\n",
    "data = data.dropna(axis=0, subset=['lat','long'])\n",
    "\n",
    "# List comprehension to make out list of lists\n",
    "heat_data = [[row['lat'],row['long']] for index, row in data.iterrows()]\n",
    "\n",
    "# Plot it on the map\n",
    "HeatMap(heat_data).add_to(fishTrackMap)\n",
    "\n",
    "# Display the map\n",
    "fishTrackMap\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
